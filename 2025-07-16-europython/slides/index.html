<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <title>The Pydantic stack for AI apps</title>

    <link rel="stylesheet" href="dist/reset.css" />
    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/pydantic.css" />

    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,100..700;1,100..700&display=swap"
      rel="stylesheet"
    />

    <!-- Theme used for syntax highlighted code -->
    <link rel="stylesheet" href="plugin/highlight/github-dark.css" />
    <style>
      .twocolumn {
        display: flex;
      }
      .col {
        flex: 1;
      }
      .circle {
        border-radius: 100px;
      }
      .small {
        font-size: 0.75em;
      }
      .left {
        text-align: left;
      }
    </style>
  </head>
  <body>
    <div class="reveal">
      <div class="slides">
        <section data-background="assets/grid.png">
          <img src="./assets/pydantic-dark.svg" alt="Pydantic" width="220px" />
          <h1 class="r-fit-text">The Pydantic stack for AI apps</h1>
          <p>EuroPython, Prague, 16 July 2025</p>
        </section>
        <section>
          <h2>Meet Laura & Victorien</h2>
          <div class="items-center">
            <img
              class="circle"
              src="./assets/laura.jpg"
              alt="Laura"
              width="200"
            />
            <img
              class="circle"
              src="./assets/victorien.jpg"
              alt="Victorien"
              width="200"
            />
          </div>
        </section>
        <section>
          <h2>The Pydantic Stack</h2>
          <img src="./assets/pydantic-stack.png" alt="Pydantic Stack" />
        </section>
        <section>
          <h2 class="r-fit-text">Agents, models, tools, MCP, A2A</h2>
          <ul>
            <li class="fragment">
              Models: Foundation providers, open source, fine-tunes
            </li>
            <li class="fragment">
              Agents: LLMs + agency (interaction with the world)
            </li>
            <li class="fragment">Tools: How agents interact with the world</li>
            <li class="fragment">MCP: Model Context Protocol</li>
            <li class="fragment">A2A: Agent-to-Agent protocol</li>
          </ul>
          <aside class="notes">
            Discuss opportunities and engineering challenges
          </aside>
        </section>
        <section>
          <h2>LLM prototype hell</h2>
          <img
            src="./assets/excitment.jpg"
            class="r-stretch"
            alt="LLM Prototype Hell"
          />
          <p class="small">- Thanks to Hugo Bowne-Anderson</p>
        </section>
        <section data-background="assets/grid.png">
          <h2>Setting the scene</h2>
          <ul>
            <li class="fragment">Docs bash week</li>
            <li class="fragment">
              Some obvious failure modes - using LLMs ad hoc, consistency of
              style, differing levels of English
            </li>
            <li class="fragment">Can we work smarter, not harder?</li>
            <li class="fragment">
              Caveat: not trying to get rid of human writers, but to lower the
              barrier to entry for developer writing
            </li>
          </ul>
          <aside class="notes">
            <p>
              A few weeks ago we did a docs bash week with the whole team. We
              had fallen behind on our docs and wanted to give them some
              dedicated attention, make sure that it was visibly prioritised.
            </p>
            <p>
              This raised some obvious questions about how we could improve our
              documentation process and ensure that we stay on track in the
              future. And also how can we make the tedious parts of writing
              easier, more reproducible while still allowing individual voices
              and ideas to shine through
            </p>
            <p>
              This led to the idea of a ghost writer agent workflow - where we
              could speed up the writing workflow, provide some guidelines and
              guard rails for different kinds of content we wanted to produce.
            </p>
          </aside>
        </section>
        <section>
          <img src="./assets/ghost-writer.jpg" alt="Ghost Writer diagram" />
          <aside class="notes">
            <p>
              This was my lazy and half baked system diagram - first cut.
              Perhaps unsurprisingly we decided to scope our project down, we
              started with blog post outputs, but we factored the project in a
              way to ensure it could scale if we wanted to add further content
              types over time.
            </p>
          </aside>
        </section>
        <section>
          <h2 class="r-fit-text">Invest in your own mental model</h2>
          <p class="fragment">Before exploring complex options, start simple</p>
          <p class="fragment">
            Focus on a specific use-case, a clear problem or pain point, and
            then expand. Don't try to boil the ocean.
          </p>
          <aside class="notes">
            <p>
              Perhaps it's obvious to say, but I think it's highly valuable to
              take the time to step back and do some simple diagrams which
              consider the overall architecture and flow of the system.
            </p>
            <p>
              Do you even need agents? DO you even need LLMs? Does your desired
              automation require consistency and determinism, or fluidity and
              "valuable" hallucinations? Rather than thinking of everything as
              starting at non-determinism, maybe reverse your mental model and
              start imaginging your system as normal computer logic, and then
              work out which inflection points must escape that determinism for
              effectiveness.
            </p>
          </aside>
        </section>
        <section>
          <h2 class="r-fit-text">When (and when not) to use agents</h2>
          <blockquote class="small left">
            "When building applications with LLMs, we recommend finding the
            simplest solution possible, and only increasing complexity when
            needed. This might mean not building agentic systems at all. Agentic
            systems often trade latency and cost for better task performance,
            and you should consider when this tradeoff makes sense."
          </blockquote>
          <p class="small">
            <a
              href="https://www.anthropic.com/engineering/building-effective-agents"
              target="_blank"
              >https://www.anthropic.com/engineering/building-effective-agents</a
            >
          </p>
        </section>
        <section>
          <img
            src="./assets/should-i-use-an-agent.jpg"
            class="r-stretch"
            alt="Agent decision flow diagram"
          />
          <p class="small">
            <a
              href="https://decodingml.substack.com/p/stop-building-ai-agents"
              target="_blank"
              >https://decodingml.substack.com/p/stop-building-ai-agents</a
            >
          </p>
        </section>
        <section>
          <h2>Building our ghost writer agent</h2>
          <aside class="notes">
            Ok so we're going to build a simple agent workflow with some inputs
            (say I want to write a blogpost about our brand redesign) and some
            output - in this case a markdown file.
          </aside>
        </section>
        <section>
          <video
            id="chatgpt-first-use"
            data-autoplay
            autoplay
            src="./assets/chatgpt_first_use.mp4"
            type="video/mp4"
          />
          <aside class="notes">
            So the first thing you would do is of course have a conversation
            about it with your favourite LLM client in a web UI or CLI. The goal
            with the GhostWriter in Pydantic AI will be to express the same
            concepts as code, in a structured way.
          </aside>
        </section>
        <section>
          <h2>The Pydantic AI framework</h2>
          <aside class="notes">
            So I'll be using the Pydantic AI Framework. I use the word framework
            because there are a lot of similarities with other web applications
            you may have developed in the past.
          </aside>
        </section>
        <section>
          <div>
            <div class="fragment">
              <div class="items-center">
                <img width="80" src="./assets/fastapi_logo.png" />
                <img width="150" src="./assets/sqlalchemy_logo.svg" />
              </div>
              <pre>
                <code data-trim class="language-python" data-line-numbers="1-3|2|3|1-3">
                  @app.get('/attendees/{id}')
                  def get_attendee(id: int) -> Attendee:
                      return select(Attendee).where(Attendee.id == id)
                </code>
              </pre>
            </div>
            <div class="fragment">
              <img width="300" src="./assets/pydantic-ai-dark.svg" />
              <pre>
                <code data-trim class="language-python" data-line-numbers="1-4|3|2|1-4">
                  agent = Agent(
                      'google-vertex:gemini-2.5-pro',
                      output_type=Annotated[int, Interval(ge=1, le=10)],
                      system_prompt='Give me a number between 1 and 10.',
                  )
                </code>
              </pre>
            </div>
          </div>
          <aside class="notes">
            <p>
              For example if you have used FastAPI web framework, Pydantic is
              actually used under the hood to do validation of the type hints
              you provide. So ID is a passed parameter, Pydantic is used to
              enforce this type - to check that it really returns an integer.
              Similarly if you've used a database framework like SQL Alchemy you
              don't have to worry about which database backend is used, whether
              it's Postgres or MySQL - you just have one unified API to query.
            </p>
            <p>
              Things are pretty similar with Pydantic AI. When we define an
              agent we use this agent class. Every agent has a specific output
              type - in this example an int between 1 and 10. And we use the
              annotated pattern from Pydantic that you may know of, that's a way
              to constrain to specific instances of generic types.
            </p>
            <p>
              Pydantic AI is model provider agnostic - all major vendors are
              available off-the-shelf. Changing model is just one line of code.
              You can also extend the existing model class to define a different
              model - if you have a fine tuned model you're hosting locally, for
              example.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2 class="r-fit-text">Implementing the ghost writer</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers="1-5|3|1-5">
                writer_agent = Agent(
                    'anthropic:claude-sonnet-4-0',
                    output_type=str,
                    instructions='You are writing a blog post for the Pydantic blog.',
                )
              </code>
            </pre>
          <aside class="notes">
            <p>
              We're going to start really simply by giving a single instruction.
              Because we want to output text we're going to use string as an
              output type.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2 class="r-fit-text">Implementing the ghost writer</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers="1-4">
                @dataclass
                class AgentDeps:
                    blog_author: str
                    reference_links: list[str]

                writer_agent = Agent(
                    'anthropic:claude-sonnet-4-0',
                    output_type=str,
                    instructions='You are writing a blog post for the Pydantic blog.',
                )
              </code>
            </pre>
          <aside class="notes">
            <p>
              And now we're going to see how we can give more context to this
              agent. Context in Pydantic AI is implemented using what we call
              dependencies. Here we have declared a simple data class where we
              will accept the blog author, and some reference links. These
              dependencies will be provided for each run of your agent.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2 class="r-fit-text">Implementing the ghost writer</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers="9|1-11">
                @dataclass
                class AgentDeps:
                    blog_author: str
                    reference_links: list[str]

                writer_agent = Agent(
                    'anthropic:claude-sonnet-4-0',
                    output_type=str,
                    deps_type=AgentDeps,
                    instructions='You are writing a blog post for the Pydantic blog.',
                )
              </code>
            </pre>
          <aside class="notes">
            <p>
              You can also specify dependencies in your agent for type checking
              purposes.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2 class="r-fit-text">Implementing the ghost writer</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers="13-17">
                @dataclass
                class AgentDeps:
                    blog_author: str
                    reference_links: list[str]

                writer_agent = Agent(
                    'anthropic:claude-sonnet-4-0',
                    output_type=str,
                    deps_type=AgentDeps,
                    instructions='You are writing a blog post for the Pydantic blog.',
                )

                await writer_agent.run(
                  'Write a blog post about...',
                  deps=AgentDeps(blog_author='Victorien', ...),
                )
              </code>
            </pre>
          <aside class="notes">
            And when you run the agent with the user prompt this is also where
            you are going to specify the dependencies.
          </aside>
        </section>
        <section data-auto-animate>
          <h2 class="r-fit-text">Implementing the ghost writer</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers>
                @writer_agent.instructions
                def add_author_info(ctx: RunContext[AgentDeps]) -> str:
                    return f"""
                    Author: {ctx.deps.blog_author}
                    Links you may query: {', '.join(ctx.deps.reference_links)}
                    """"
              </code>
            </pre>
          <aside class="notes">
            <p>
              In order for the LLM to make use of these dependencies we make use
              of what's called dynamic instructions. Here you can see a
              decorator pattern - this is quite common in Pydantic AI, we are
              trying to reuse existing concepts in Python so you don't have to
              learn too much DLS to work with our framework.
            </p>
            <p>
              More generally we really are trying to keep the same feeling using
              Pydantic and Pydantic AI - wherever possible we use the same
              concepts, same APIs, same type safety.
            </p>
            <p>
              So in this case the instructions are going to use the dependencies
              to provide additional instructions: author and any links the model
              may query.
            </p>
          </aside>
        </section>
        <section>
          <h2>System Prompts / Instructions</h2>
          <img
            src="./assets/system-prompts.gif"
            alt="System prompt architecture"
          />
          <aside class="notes">
            <p>
              Of course we want to provide more context to the model using
              system prompts. In Pydantic AI there are two concepts available to
              use - System Prompts and Instructions. Instructions are the
              initial message sent to the LLM - sent per run and specific to the
              current agent. This is similar to, but different from, system
              prompts, where your request to the model retains the system
              prompts used in previous requests. In general we recommend using
              Instructions.
            </p>
            <p>
              In this case we are simply accessing the agent specific prompts
              plus the shared prompts to create a tailored full prompt for the
              agent. While we could easily make this a monolithic
              prompt-per-agent, we factored it this way to make it easy to be
              able to run evals and do prompt engineering experiments.
            </p>
            <p>
              Maybe one of these is totally useless? We factored it in a way
              that made sense to us conceptually, and would be easy to turn on
              and off prompts accordingly.
            </p>
          </aside>
        </section>
        <section>
          <h2>Our prompt factoring</h2>
          <div class="r-stack">
            <img
              class="fragment"
              class="r-stretch"
              src="./assets/prompt-global-styleguide.png"
              alt="Global style guide"
            />
            <img
              class="fragment"
              class="r-stretch"
              src="./assets/prompt-vocabulary.png"
              alt="Global vocabulary"
            />
            <img
              class="fragment"
              class="r-stretch"
              src="./assets/prompt-writer-blogpost.png"
              alt="Writer specific prompt"
            />
            <img
              class="fragment"
              class="r-stretch"
              src="./assets/prompt-reviewer-blogpost.png"
              alt="Reviewer specific prompt"
            />
          </div>
        </section>
        <section>
          <img
            class="r-stretch"
            src="./assets/prompt-strunk-and-whites.png"
            alt="Strunk and Whites Elements of Style"
          />
          <aside class="notes">
            <p>
              This is an example of another resource we could add to the
              available system prompts and experiment with model output quality.
              I haven't tried it yet but I'm curious to see how effective adding
              a short book (~80 pages) might be to tuning the language of the
              output further.
            </p>
            <p>
              We actually implemented our system prompt builder as a tool, in
              order to make the logic clear and reusable. So let's talk about
              tool calling in Pydantic AI.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2>Introducing tools</h2>
          <aside class="notes">
            <p>So we talked about dependencies and instructions.</p>
            <p>
              Dependencies are dependency injection system to provide data and
              services to your agent's system prompts, tools and output
              validators.
            </p>
            <p>
              Instructions and System Prompts are very similar with Instructions
              being sent per run and specific to the current agent, and system
              prompts preserving system prompt messages in the message history
              sent to the LLM in subsequent completions requests.
            </p>
            <p>
              But of course the other concept we need to make our agents
              agentive: Tools they can call. We are using models which are
              reasonably recent but with well structured cut-off dates for their
              training on recent web pages. So we actually want to give more
              recent information to the Agent, and let the Agent interact with
              other systems in the world. And for this we use Tools.
            </p>
          </aside>
        </section>
        <section data-auto-animate>
          <h2>Introducing tools</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers="1-12|2">
                @agent.tool_plain
                async def extract_technical_content(url: HttpUrl) -> str:
                    """Extract technical content from one of the reference links."""

                    async with httpx.AsyncClient() as client:
                      response = client.get(str(url))

                    return trafilatura.extract(
                        response.text,
                        output_format="html",
                        favor_precision=True,
                        include_tables=True,
                    )
              </code>
            </pre>
          <aside class="notes">
            <p>
              Similar to dynamic Instructions, we use a decorator pattern for
              this. A tool is simply a function that the Agent may or may not
              decide to call - and its python code so its really powerful, you
              can pretty much do anything you want.
            </p>
            <p>
              You could have a database and run sql queries on it to fetch
              custom log data, if you were writing a chatbot, for example.
            </p>
            <p>
              In our case I mentioned reference links that the Agent may query
              as it writes the blog post. We actually use an http client to make
              the request, and we this Trafilatura library which can extract
              content from web pages and has a specialty in extracting and
              structuring code and technical content in a way that's easy for
              the LLM to understand.
            </p>
            <p>
              Again here we have specific arguments that can be provided by the
              LLM and we want Pydantic validation to be hooked up here. So if
              the LLM decides for some reason to give something that is not the
              link to be queried, Pydantic will raise a validation error.
              PydanticAI will raise that validation error to the Agent with a
              thin wrapper to the effect of "hey you made a mistake here, we
              want a URL that can be queried, please try again".
            </p>
          </aside>
        </section>
        <section>
          <h2>Quick recap</h2>
          <pre data-id="code-animation">
              <code data-trim class="language-python" data-line-numbers>
                writer_agent = Agent(
                    'anthropic:claude-sonnet-4-0',
                    output_type=str,
                    deps_type=AgentDeps,
                    instructions='You are writing a blog post for the Pydantic blog.',
                )

                @writer_agent.instructions
                def add_author_info(ctx: RunContext[AgentDeps]) -> str:
                    ...

                @writer_agent.tool_plain
                def extract_technical_content(url: HttpUrl) -> str:
                    ...
              </code>
            </pre>
          <aside class="notes"></aside>
        </section>
        <section data-auto-animate>
          <h2>Introducing a reviewer agent</h2>
          <pre data-id="code-animation">
            <code data-trim class="language-python" data-line-numbers="1-12|1-5">
              class Review(BaseModel):
                  score: Annotated[int, Field(ge=1, le=10)]

                  feedback: str
                  """Feedback on what could be improved."""


              reviewer_agent = Agent(
                  'anthropic:claude-sonnet-4-0',
                  output_type=Review,
                  instructions='Review and provide a score for a blog post.',
              )
            </code>
            <aside class="notes">
              <p>A quick recap of what we have now:</p>
              <ul>
                <li>Agent definition </li>
                <li></li>
              </ul>

            </aside>
          </pre>
        </section>
        <section data-auto-animate>
          <h2>Introducing a reviewer agent</h2>
          <pre data-id="code-animation">
            <code data-trim class="language-python" data-line-numbers=13-16|1-16>
              class Review(BaseModel):
                  score: Annotated[int, Field(ge=1, le=10)]

                  feedback: str
                  """Feedback on what could be improved."""

              reviewer_agent = Agent(
                  'anthropic:claude-sonnet-4-0',
                  output_type=Review,
                  instructions='Review and provide a score for a blog post.',
              )

              @writer_agent.tool_plain
              async def review_blog_content(content: str) -> Review:
                  result = await rewiewer_agent.run(f'Review:\n\n{content}')
                  return result.output
            </code>
          </pre>
          <aside class="notes"></aside>
        </section>

        <section>
          <img class="r-stretch" src="./assets/diagram-claude1.png" />
        </section>
        <section>
          <img class="r-stretch" src="./assets/diagram-claude2.png" />
        </section>
        <section>
          <img class="r-stretch" src="./assets/diagram1.svg" />
        </section>
        <section>
          <img class="r-stretch" src="./assets/diagram2.svg" />
        </section>
        <section>
          <img class="r-stretch" src="./assets/diagram3.svg" />
        </section>
        <section>
          <img class="r-stretch" src="./assets/diagram4.svg" />
        </section>
        <section>
          <h2>Demo time...</h2>
        </section>
        <section data-background="assets/grid.png">
          <h3>Thank you!</h3>
          <img
            src="./assets/talkqr.png"
            alt="QR code to get the demo and slides"
            class="r-stretch"
          />
          <p class="small left">p.s. AI won't write all your content</p>
          <p class="small left">
            p.p.s. Come see us at our sponsor booth, we would love to dig into
            this code in more detail and answer any questions!!
          </p>
        </section>
        <aside class="notes"></aside>
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/notes/notes.js"></script>
    <script src="plugin/markdown/markdown.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script>
      // More info about initialization & config:
      // - https://revealjs.com/initialization/
      // - https://revealjs.com/config/
      Reveal.initialize({
        hash: true,

        // Learn about plugins: https://revealjs.com/plugins/
        plugins: [RevealMarkdown, RevealHighlight, RevealNotes],
      });

      const chatgptVideo = document.getElementById("chatgpt-first-use");
      chatgptVideo.playbackRate = 2.0;
    </script>
  </body>
</html>
